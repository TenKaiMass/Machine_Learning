{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "binPN-FDOmt4"
   },
   "source": [
    "# Mod√®le lin√©aire\n",
    "\n",
    "Consid√©rons la cas classique d'une fonction affine :\n",
    "\n",
    "$$y=ax+b$$\n",
    "\n",
    "Ici, $a$ et $b$ sont des r√©els. Ces deux nombres d√©finissent enti√®rement la courbe et permet donc d'obtenir une relation **affine** entre $x$ et $y$. En statistique, cette relation est √† la base des mod√®les dit **lin√©aires**, o√π une variable r√©ponse se d√©finit comme une somme de variables explicatives o√π chacune de ces derni√®res sont multipli√©s par un coefficient.\n",
    "\n",
    "\n",
    "## Mod√®le lin√©aire simple\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/438px-Linear_regression.svg.png)\n",
    "\n",
    "Dans le mod√®le lin√©aire simple (une seule variable explicative), on suppose que la variable r√©ponse suit le mod√®le suivant :\n",
    "\n",
    "$$y_i=\\beta_0 + \\beta_1 x_i + \\varepsilon_i$$\n",
    "\n",
    "On remarque la ressemblance avec la fonction affine pr√©sent√©e ci-dessus. La diff√©rence r√©side dans l'existence du terme al√©atoire (appel√© bruit) $\\varepsilon_i$. Afin de consid√©rer le mod√®le, il est n√©cessaire de se placer sous les hypoth√®ses suivantes\n",
    "\n",
    "$$(\\mathcal{H}) : \\left\\{\\begin{matrix}\n",
    "\\mathbb{E}[\\varepsilon_i]=0\\\\ \n",
    "\\text{Cov}(\\varepsilon_i, \\varepsilon_j)=\\delta_{ij} \\sigma^2\n",
    "\\end{matrix}\\right.$$\n",
    "Les diff√©rents √©l√©ments qui interviennent sont :\n",
    "\n",
    "- $\\beta_0$ : l'ordonn√©e √† l'origine (nomm√©e *intercept*)\n",
    "- $\\beta_1$ : le coefficient directeur\n",
    "- $x_i$ : l'observation $i$\n",
    "- $y_i$ : le $i$-√®me prix\n",
    "- $\\varepsilon_i$ : le bruit al√©atoire li√©e √† la $i$-√®me observation\n",
    "\n",
    "La solution peut se calculer facilement via les formules ferm√©es suivantes :\n",
    "\n",
    "$$\\hat{\\beta}_1=\\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\qquad \\hat{\\beta}_0 = \\hat{y} - \\hat{\\beta}_1 \\bar{x}$$\n",
    "\n",
    "##¬†Mod√®le lin√©aire multiple\n",
    "\n",
    "Dans le cas multiple (pour $p$ variables explicatives), pour la $i$-√®me observation, le mod√®le s'√©crit :\n",
    "\n",
    "$$y_i= \\beta_0 + \\sum_{j=1}^p \\beta_j x_{ij} + \\varepsilon_i$$\n",
    "\n",
    "Ainsi, une observation $x_i$ n'est plus une valeur, mais un **vecteur** $(x_{i1}, \\dots, x_{ip})$. Il est plus commode de regrouper ces prix $y_i$ et ces vecteurs d'observations $x_i$ dans des matrices :\n",
    "\n",
    "$$Y=X \\beta + \\varepsilon$$\n",
    "\n",
    "Sous les hypoth√®ses √©quivalentes du mod√®le simple en plus grand dimension\n",
    "\n",
    "$$(\\mathcal{H}) : \\left\\{\\begin{matrix}\n",
    "\\text{rank}(X)=p\\\\ \n",
    "\\mathbb{E}[\\varepsilon]=0 \\text{ et }\\text{Var}(\\varepsilon)=\\sigma^2 I_p\n",
    "\\end{matrix}\\right.$$\n",
    "\n",
    "Les diff√©rents √©l√©ments qui interviennent sont :\n",
    "\n",
    "- $\\beta$ : le vecteur directeur\n",
    "- $X$ : la matrice des observations\n",
    "- $Y$ : le vecteur de prix\n",
    "- $\\varepsilon$ : le vecteur de bruit\n",
    "\n",
    "Avec $X=( \\mathbf{1}, X_1, \\dots, X_n)$, $Y=(y_1, \\dots, y_n)^\\top$ et $\\varepsilon=(\\varepsilon_1, \\dots, \\varepsilon_n)^\\top$. La solution des MCO (Moindres Carr√©s Ordinaires) est alors :\n",
    "\n",
    "$$\\hat{\\beta}= (X^\\top X)^{-1} X^\\top Y$$\n",
    "\n",
    "Vous pouvez d'ailleurs faire la d√©monstration de votre cot√© ! Pour plus d'information math√©matiques, le portail de wikip√©dia qui est tr√®s bien fait : [lien ici](https://fr.wikipedia.org/wiki/Portail:Probabilit%C3%A9s_et_statistiques)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dRQO2ydTOmuA"
   },
   "source": [
    "# Impl√©menter une r√©gression lin√©aire \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ckyBi2FLOmuE"
   },
   "outputs": [],
   "source": [
    "#importer vos librairies \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B6I-a12SOmud",
    "outputId": "cbe1bdef-5dda-429a-a4f9-81fc4df45794"
   },
   "outputs": [],
   "source": [
    "#charger les donn√©es \n",
    "data_price = pd.read_csv(\"./dataset/price_availability.csv\",sep=\";\")\n",
    "data_listing = pd.read_csv(\"./dataset/listings_final.csv\",sep=\";\")\n",
    "#price_availability.csv\n",
    "#listings_final.csv\n",
    "#v√©rifier si tous les individus ont bien un prix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XT3LsnvVOmut"
   },
   "source": [
    "## Donn√©es d'entr√©e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T8DTTFEKOmu0"
   },
   "source": [
    "L'objectif ici est de charger les donn√©es pour cr√©er les matrices $X$ et $Y$ du mod√®le lin√©aire. **Attention**, il n'est pas n√©cessaire de rajouter le vecteur colonne $\\mathbf{1}$ en premi√®re colonne, car *scikit-learn* le fait automatiquement !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tnTd7XKdOmu5"
   },
   "outputs": [],
   "source": [
    "#fusion des dataset\n",
    "data_fusion = pd.merge(data_listing,data_price, on=\"listing_id\")\n",
    "data_final = data_fusion.groupby('listing_id').mean()\n",
    "\n",
    "#d√©finir 2 variables de travail\n",
    "#X := les features √† utiliser \n",
    "X = data_final[['pricing_weekly_factor','pricing_monthly_factor']]\n",
    "#Y := la target (prix)\n",
    "Y = data_final['local_price']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test\n",
    "# #data_top_annonce= \n",
    "# data_listing.nlargest(3,'beds')\n",
    "# data_price[data_price['listing_id']== 18691947]\n",
    "# # data_listing['beds'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_zalIbpOmvI"
   },
   "outputs": [],
   "source": [
    "#construire l'ensemble de donn√©e prix \n",
    "#\n",
    "#    INDICE \n",
    "# \n",
    "# r√©cup√©rer les prix des ID dans le dataset de prix \n",
    "# üöß il y a plusieurs prix dans le dataset üöß\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z1ckoHuzOmvW"
   },
   "source": [
    "En *Machine Learning*, on a l'habitude de couper l'ensemble de donn√©es en deux sous-ensembles :\n",
    "\n",
    "- Un ensemble d'entra√Ænement (*train set*), sur lequel le mod√®le va √™tre calibr√©.\n",
    "- Un ensemble de test (*test set*), qui ne sera pas utilis√© pendant le calibrage mais permettra de v√©rifier l'aptitude du mod√®le √† g√©n√©raliser sur de nouvelles observations inconnues.\n",
    "\n",
    "En g√©n√©ral, on d√©coupe l'ensemble de donn√©es (*split*) en prenant $\\alpha \\%$ de l'ensemble pour entra√Ænement et $1-\\alpha \\%$ comme test. Dans la plus part des cas, on consid√®re que $\\alpha=10,20 ou 30\\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-x-APn4OmvX",
    "outputId": "ff55319f-f6d2-477c-811e-f1eb2d5ef7b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09447732915482576"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#utiliser la m√©thode split de sklearn en splitant avec un alpha=30 et un random state=42 \n",
    "#zafficher la shape de vos donn√©es \n",
    "model = GradientBoostingRegressor(alpha=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "szd_JPzXOmvq"
   },
   "source": [
    "## Entra√Ænement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1e4cfWxoOmvr"
   },
   "source": [
    "Pour information, *scikit-learn* utilise le solveur OLS (Ordinary Least Squares) de *numpy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TrS4fboZOmvu",
    "outputId": "9a26f504-379b-45b3-b498-8b37befc4e30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.3, random_state=42)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cr√©er l'objet de r√©gression et entrainer le sur notre ensemble d'entra√Ænement\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "40mJxNwhOmv5"
   },
   "source": [
    "On affiche le vecteur des coefficients pour interpr√©ter rapidement le mod√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uUjLZYBEOmv7",
    "outputId": "3ac3800a-e028-46d6-e367-f64e1232d3bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09447732915482576"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#afficher les coefficients\n",
    "model.score(X,Y)\n",
    "#que remarquez vous ? \n",
    "reponse = \"il est a 0.0944\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nt4z_sh4OmwI"
   },
   "source": [
    "## Validation du mod√®le\n",
    "\n",
    "### Le coefficient de d√©termination $R^2$\n",
    "\n",
    "Par la suite, nous ferons l'hypoth√®se de gaussianit√© sur les bruits. Dans l'id√©e, nous aimerions obtenir une valeur num√©rique qui nous indique √† quel point la r√©gression lin√©aire a un sens sur nos donn√©es. Pour cela, introduisons les notations suivantes :\n",
    "\n",
    "- $SCT=\\|Y-\\hat{y} \\mathbf{1}\\|^2$ est la somme des carr√©s totaux\n",
    "- $SCE=\\|\\hat{Y}-\\hat{y} \\mathbf{1}\\|^2$ est la somme des carr√©s expliqu√©s\n",
    "- $SCR=\\|\\hat{\\varepsilon}\\|^2$ est la somme des carr√©s r√©siduels\n",
    "\n",
    "L'id√©e est de d√©composer la somme des carr√©s totaux comme la somme des carr√©s que le mod√®le explique, en plus de la somme des carr√©s qui sont li√©s aux r√©sidus (et donc que le mod√®le ne peut pas expliquer). On voit donc ici l'int√©r√™t de calculer un coefficient √† partir du $SCE$. Puisque l'on a la relation suivante :\n",
    "\n",
    "$$SCT=SCE+SCR \\text{ alors } 1=\\frac{SCE}{SCT}+\\frac{SCR}{SCT}$$\n",
    "\n",
    "Plus les r√©sidus sont petits (et donc la r√©gression est \"bonne\"), plus $SCR$ devient petit et donc $SCE$ devient grand. Le sch√©ma inverse s'op√®re de la m√™me fa√ßon. Dans le meilleur des cas, on obtient $SCR=0$ et donc $SCE=SCT$ d'o√π le premier membre vaut $1$. Dans le cas contraite, $SCE=0$ et automatiquement, le premier membre est nul. C'est ainsi que l'on d√©finit le coefficient de d√©termination $R^2$ comme \n",
    "$$R^2=\\frac{SCE}{SCT}=1-\\frac{SCR}{SCT}$$\n",
    "Ainsi, $R^2 \\in [0,1]$. Plus $R^2$ est proche de $1$, plus la r√©gression lin√©aire a du sens. Au contraire, si $R^2$ est proche de $0$, le mod√®le lin√©aire poss√®de un faible pouvoir explicatif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uf8_Sw5yOmwJ",
    "outputId": "5035a388-7cdb-4346-f833-d47fdabf2bc0"
   },
   "outputs": [],
   "source": [
    "#faire une prediction sur X\n",
    "prediction = model.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AEQrrOb6OmwV",
    "outputId": "f5e692a4-e10f-4a15-d37a-a12edfc7aaaf"
   },
   "outputs": [],
   "source": [
    "#afficher l'erreur des moindres carr√©es sur l'ensemble d'entrainement ainsi que le R2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3C3r6tz9Omwd"
   },
   "source": [
    "## Bonus : Analyse de l'homosc√©dasticit√©\n",
    "\n",
    "L'analyse de l'homosc√©dasticit√© est primordiale : c'est en particulier elle qui nous permet de v√©rifier, √† partir des r√©sidus, si les bruits v√©rifient bien l'hypoth√®se $(\\mathcal{H})$. On calcule donc les **r√©sidus studentis√©s**.\n",
    "\n",
    "$$t_i^*=\\frac{\\hat{\\varepsilon}_i}{\\hat{\\sigma}_{(i)} \\sqrt{1-h_{ii}}}$$\n",
    "Avec $h_{ii}=\\{X(X^\\top X)^{-1} X^\\top\\}_{ii}=H_{ii}$ la matrice de projection sur l'hyperplan des variables. Plus pr√©cis√©ment, $H$ est la matrice qui projette $Y$ sur l'espace engendr√© par les variables, soit $\\hat{Y}=HY$. De m√™me, on consid√®re $\\hat{\\sigma}_{(i)}$ l'estimateur de la variance du bruit en supprimant l'observation $i$ (par une m√©thode de validation crois√©e Leave-One-Out que nous ne d√©taillerons pas ici).\n",
    "\n",
    "Dans ce cas, on peut montrer que les r√©sidus studentis√©s suivent une loi de Student √† $n-p-1$ degr√©s de libert√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QC71Z7HhOmwf",
    "outputId": "ab8657d8-81d0-4c77-871a-d232178a2537"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'regr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4964/2206579234.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#analyser le code ci-dessous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'regr' is not defined"
     ]
    }
   ],
   "source": [
    "#analyser le code ci-dessous \n",
    "import scipy\n",
    "Y_pred = regr.predict(X_train)\n",
    "n = X_train.shape[0]\n",
    "p = 4\n",
    "residuals = np.abs(y_train - Y_pred)\n",
    "H = np.matmul(X_train, np.linalg.solve(np.dot(X_train.T, X_train), X_train.T))\n",
    "std_hat = np.dot(residuals, residuals) / (n - p)\n",
    "standart_residuals = np.asarray([residuals[i] / np.sqrt(std_hat * (1 - H[i, i])) for i in range(len(residuals))])\n",
    "student_residuals = np.asarray([ standart_residuals[i] * np.sqrt((n - p - 1) / (n - p - standart_residuals[i]**2)) for i in range(n) ])\n",
    "cook = np.asarray([ H[i, i] * student_residuals[i] / (X_train.shape[1] * (1 - H[i, i])) for i in range(n) ])\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.subplot(221)\n",
    "plt.scatter(Y_pred, student_residuals, s=12, c=\"white\", edgecolors=\"blue\")\n",
    "plt.plot([min(Y_pred), max(Y_pred)], [ scipy.stats.t.ppf(q=0.975, df=n-p-1), scipy.stats.t.ppf(q=0.975, df=n-p-1)], color=\"green\", alpha=0.6, label=\"Quantile de Student\")\n",
    "plt.title(\"Analyse de l‚Äôhomosc√©dasticit√©\")\n",
    "plt.xlabel(\"Pr√©dictions $\\hat{y}_i$\")\n",
    "plt.ylabel(\"R√©sidus studentis√©s $|t_i^*|$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RHy-a9jROmws"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "ModeleLineaire1.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
